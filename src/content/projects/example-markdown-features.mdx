---
title: "Building a Recommendation System from Scratch"
date: "2024-02-20"
description: "Complete guide to building a collaborative filtering recommendation system with Python"
tags: ["machine-learning", "recommendation-systems", "python", "data-science"]
github: "https://github.com/yourusername/recommendation-system"
demo: "https://rec-system-demo.herokuapp.com"
---

# Building a Recommendation System from Scratch

A comprehensive project building a movie recommendation system using collaborative filtering and matrix factorization techniques.

## Project Overview

This project demonstrates how to build a production-ready recommendation system from data collection to deployment. We'll cover:

- Data preprocessing and exploration
- Multiple recommendation algorithms
- Model evaluation and comparison
- Deployment considerations

## Data Collection

We used the **MovieLens 100K dataset** containing:
- 100,000 ratings
- 943 users
- 1,682 movies
- Ratings on a 1-5 scale

### Loading the Data

```python
import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix

# Load ratings data
ratings = pd.read_csv('ratings.csv')
print(ratings.head())

# Output:
#    userId  movieId  rating  timestamp
# 0       1        1     4.0  964982703
# 1       1        3     4.0  964981247
```

## Exploratory Data Analysis

### Rating Distribution

> "Understanding your data is the first step to building a good model."

Key findings:
- **Most common rating**: 4 stars (34.2%)
- **Average rating**: 3.53 stars
- **Rating sparsity**: 93.7% (users rate very few movies)

### User Activity

Some interesting statistics:

1. **Most active user**: 737 ratings
2. **Least active user**: 20 ratings
3. **Average ratings per user**: 106.5

## Algorithms Implemented

### 1. User-Based Collaborative Filtering

Finds similar users and recommends items they liked:

```python
from sklearn.metrics.pairwise import cosine_similarity

def get_user_similarity(user_item_matrix):
    """
    Calculate cosine similarity between users
    
    Args:
        user_item_matrix: sparse matrix of shape (n_users, n_items)
    
    Returns:
        similarity_matrix: array of shape (n_users, n_users)
    """
    # Normalize by user mean
    user_mean = user_item_matrix.mean(axis=1)
    user_item_centered = user_item_matrix - user_mean.reshape(-1, 1)
    
    # Compute cosine similarity
    similarity = cosine_similarity(user_item_centered)
    
    return similarity

# Example usage
user_similarity = get_user_similarity(user_item_matrix)
print(f"User similarity shape: {user_similarity.shape}")
```

### 2. Matrix Factorization (SVD)

Decomposes the user-item matrix into latent factors:

```python
from scipy.sparse.linalg import svds

def matrix_factorization_svd(ratings_matrix, k=50):
    """
    Perform SVD on ratings matrix
    
    Args:
        ratings_matrix: user-item rating matrix
        k: number of latent factors
    
    Returns:
        predictions: predicted ratings matrix
    """
    # Subtract user mean
    user_mean = ratings_matrix.mean(axis=1)
    ratings_centered = ratings_matrix - user_mean.reshape(-1, 1)
    
    # SVD
    U, sigma, Vt = svds(ratings_centered, k=k)
    sigma = np.diag(sigma)
    
    # Reconstruct predictions
    predictions = np.dot(np.dot(U, sigma), Vt) + user_mean.reshape(-1, 1)
    
    return predictions

# Train model
predictions = matrix_factorization_svd(train_matrix, k=50)
```

### 3. Neural Collaborative Filtering

Deep learning approach using embeddings:

```python
import torch
import torch.nn as nn

class NCF(nn.Module):
    def __init__(self, n_users, n_items, embedding_dim=50):
        super(NCF, self).__init__()
        
        # User and item embeddings
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.item_embedding = nn.Embedding(n_items, embedding_dim)
        
        # MLP layers
        self.fc_layers = nn.Sequential(
            nn.Linear(embedding_dim * 2, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
    def forward(self, user_ids, item_ids):
        # Get embeddings
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        # Concatenate
        x = torch.cat([user_emb, item_emb], dim=1)
        
        # Pass through MLP
        output = self.fc_layers(x)
        
        return output.squeeze()
```

## Evaluation Metrics

We evaluated models using multiple metrics:

### Root Mean Square Error (RMSE)

```python
def rmse(predictions, targets):
    """Calculate RMSE between predictions and targets"""
    return np.sqrt(np.mean((predictions - targets) ** 2))
```

### Mean Average Precision @ K (MAP@K)

```python
def average_precision_at_k(actual, predicted, k=10):
    """
    Calculate average precision @ k for a single user
    
    Args:
        actual: list of relevant items
        predicted: list of recommended items (ordered by score)
        k: number of recommendations to consider
    """
    if not actual:
        return 0.0
    
    predicted_k = predicted[:k]
    
    score = 0.0
    num_hits = 0.0
    
    for i, p in enumerate(predicted_k):
        if p in actual:
            num_hits += 1.0
            score += num_hits / (i + 1.0)
    
    return score / min(len(actual), k)
```

## Results Comparison

| Model | RMSE | MAP@10 | Training Time |
|-------|------|--------|---------------|
| User-Based CF | 0.98 | 0.42 | 5 min |
| SVD (k=50) | 0.92 | 0.51 | 12 min |
| SVD (k=100) | **0.89** | 0.53 | 25 min |
| Neural CF | 0.91 | **0.56** | 45 min |

**Key Takeaways:**
- Neural CF achieved best MAP@10 (recommendation quality)
- SVD with k=100 achieved best RMSE (rating prediction)
- User-Based CF was fastest but least accurate

## Deployment Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Client    â”‚
â”‚  (Web App)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flask API      â”‚
â”‚  - Recommend     â”‚
â”‚  - Rate item     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Redis   â”‚  â”‚ PostgreSQL â”‚
â”‚  (Cache)  â”‚  â”‚  (Ratings) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Example

```python
from flask import Flask, jsonify, request

app = Flask(__name__)

@app.route('/recommend/<int:user_id>', methods=['GET'])
def recommend(user_id):
    """Get recommendations for a user"""
    k = request.args.get('k', 10, type=int)
    
    # Check cache
    cache_key = f"recommendations:{user_id}:{k}"
    cached = redis.get(cache_key)
    
    if cached:
        return jsonify(json.loads(cached))
    
    # Generate recommendations
    recommendations = model.predict(user_id, top_k=k)
    
    # Cache for 1 hour
    redis.setex(cache_key, 3600, json.dumps(recommendations))
    
    return jsonify(recommendations)
```

## Challenges & Solutions

### Challenge 1: Cold Start Problem

**Problem**: New users have no rating history

**Solutions**:
- Ask for initial preferences during onboarding
- Use content-based features (genre, actors)
- Start with popularity-based recommendations

### Challenge 2: Scalability

**Problem**: Similarity computation is O(nÂ²) for n users

**Solutions**:
- Use approximate nearest neighbors (Annoy, FAISS)
- Pre-compute similarities offline
- Implement caching strategies

### Challenge 3: Bias & Fairness

**Problem**: Popular items dominate recommendations

**Solutions**:
- Add diversity metrics to evaluation
- Implement re-ranking for diversity
- Consider fairness constraints

## Future Improvements

Things I'd like to add:

- [ ] Context-aware recommendations (time, location)
- [ ] Multi-armed bandit for exploration
- [ ] Hybrid approach combining CF and content-based
- [ ] Real-time model updates
- [ ] A/B testing framework

## Lessons Learned

1. **Simple models work well**: User-based CF provided decent results with minimal complexity
2. **Cold start is hard**: Handling new users/items requires careful design
3. **Evaluation is tricky**: Offline metrics don't always correlate with user satisfaction
4. **Deployment matters**: Caching and optimization are crucial for production

## Resources

Here are some helpful resources:

- [MovieLens Dataset](https://grouplens.org/datasets/movielens/)
- [Surprise Library](http://surpriselib.com/) for collaborative filtering
- [LightFM](https://github.com/lyst/lightfm) for hybrid recommendations
- Book: "Recommender Systems: The Textbook" by Charu Aggarwal

## Code Repository

Full implementation available on GitHub:
ğŸ‘‰ [github.com/yourusername/recommendation-system](https://github.com/yourusername/recommendation-system)

---

**Built with**: Python, PyTorch, Flask, Redis, PostgreSQL

*This project was a great learning experience in building end-to-end ML systems!*

